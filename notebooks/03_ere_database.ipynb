{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41cbd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jason Chiappa 2025\n",
    "# This program creates a preliminary database of extreme precipitation events from the Stage IV dataset.\n",
    "# EREs defined as a single or group of 12 hr accumulation grid points exceeding the 10-yr ARI threshold from NOAA Atlas 14.\n",
    "# A csv file of event attributes, numpy arrays of maximum 12-hr precip, and accumulation map images are exported.\n",
    "# Note: Takes around 2 hours per year of data\n",
    "\n",
    "# 2025 version UPDATE:\n",
    "# Uses an improved approach on gathering extreme events compared to the 2024 version (used for Chiappa et al. (2024) GRL paper)\n",
    "# Fixes bug that causes certain simultaneous events/exceedance points to be excluded from the final database\n",
    "# Improves on previous method by combining all events with shared exceedance points during this process instead of in post-processing\n",
    "# This method helps prevent extreme 12-hr accumulations from being left out of the database\n",
    "# Instead of just 1 specific event's 12-hr accumulation map, records the *maximum* 12-hr accumulation within a wider timeframe (when applicable)\n",
    "# Adds parameters: earliest accumulation window start time, latest accumulation window end time\n",
    "# Exports exceedance arrays and event map figures within program instead of during post-processing\n",
    "# This version takes longer to run, but eliminates longer post-processing\n",
    "# Precautions are in place for memory errors, but latest version makes them unlikely to occur\n",
    "\n",
    "# Future research should implement the NOAA Atlas 15 dataset for thresholds when available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c07186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "from scipy import interpolate\n",
    "from math import ceil, floor\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import shapely.geometry as sgeom\n",
    "from cartopy.geodesic import Geodesic\n",
    "import gc\n",
    "import psutil\n",
    "from IPython.display import clear_output\n",
    "import time as timemod\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7d16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "#### PLEASE MODIFY AS NEEDED ####\n",
    "##############################################################################################\n",
    "\n",
    "# Distance by which to separate simultaneous events (km) - i.e. radial distance from max accumulation\n",
    "sep_dist = 250\n",
    "# sep_dist = 100\n",
    "\n",
    "# Original sep_dist = 250 km\n",
    "# Recommended sep_dist = 100 km for future use\n",
    "# This will create a larger database but will not merge simultaneous events with a relatively large separation.\n",
    "\n",
    "# IMPORTANT!!!\n",
    "# True if resuming (or adding additional data to append to dataset), false if starting from beginning.\n",
    "# Failing to change to True will overwrite all previous data! Be extremely careful not to leave this as 'False'!\n",
    "resume = False\n",
    "\n",
    "# True if resuming after a memory error\n",
    "resume_from_memory_error = False\n",
    "# no need to change values below if True (will call correct values to resume from from memory error log)\n",
    "\n",
    "# TIME RANGE for this run (YYYY-MM-DD HH:MM:SS) [see last entry in log file if debugging an error]\n",
    "start_time = '2002-01-01 00:00:00'\n",
    "end_time = '2024-12-31 23:00:00'\n",
    "\n",
    "# Initialize lat/lon grid version id (the grid changes slightly through the dataset)\n",
    "# latver = 1 if resume = False, else refer to last latver in previous run (see log file)\n",
    "latver = 1\n",
    "\n",
    "# Main data directory (for input and output of small files)\n",
    "# Must include states_21basic folder (see README)\n",
    "datadir = 'C:/Users/jchiappa/OneDrive - University of Oklahoma/ERE_analysis/v2/data/'\n",
    "\n",
    "# st4 1h data directory\n",
    "path01 = 'E:/Research/EREs/data/stage_iv/01h/'\n",
    "\n",
    "# st4 6h data directory\n",
    "path06 = 'E:/Research/EREs/data/stage_iv/06h/'\n",
    "\n",
    "# st4 24h data directory\n",
    "path24 = 'E:/Research/EREs/data/stage_iv/24h/'\n",
    "\n",
    "# NOAA Atlas 14 data directory\n",
    "atlas_dir = 'E:/Research/EREs/data/noaa_atlas_14/'\n",
    "\n",
    "# directory for lat/lon grids\n",
    "latlon_dir = 'E:/Research/EREs/data/stageiv_latlon/'\n",
    "\n",
    "# directory of US masks\n",
    "usmask_dir = 'E:/Research/EREs/data/us_masks/'\n",
    "\n",
    "# Map array export directory (will include folders for 12-hr accumulation maps and 1-hr maps at peak accumulation hours)\n",
    "# Make sure to remove old version output or change the path first\n",
    "arr_dir = 'E:/Research/EREs/data/array_output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3537223-37dd-46c9-a514-17d5fd5ed226",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "#### Leave the stuff below ####\n",
    "##############################################################################################\n",
    "\n",
    "if resume_from_memory_error:\n",
    "    start_time,latver = np.load(datadir+'mem_error_log.npy')\n",
    "    start_time = str(start_time)\n",
    "    latver = int(latver)\n",
    "\n",
    "# directory for all array output\n",
    "arr_dir += str(sep_dist)+'km/'\n",
    "\n",
    "for fdir in [arr_dir,latlon_dir]:\n",
    "    if (resume==False) & (os.path.exists(fdir)): # wipe all existing data!\n",
    "        import shutil\n",
    "        def delete_all_files_in_directory(directory):\n",
    "            for filename in os.listdir(directory):\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                        os.unlink(file_path)\n",
    "                    elif os.path.isdir(file_path):\n",
    "                        shutil.rmtree(file_path)\n",
    "                except Exception as e:\n",
    "                    print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "        delete_all_files_in_directory(fdir)\n",
    "\n",
    "# create directories if they do not exist\n",
    "os.makedirs(arr_dir,exist_ok=True)\n",
    "os.makedirs(latlon_dir,exist_ok=True)\n",
    "os.makedirs(usmask_dir,exist_ok=True)\n",
    "os.makedirs(atlas_dir,exist_ok=True)\n",
    "\n",
    "# 12-hr accumulation map directory\n",
    "accum_path = arr_dir+'accum_12h/'\n",
    "os.makedirs(accum_path,exist_ok=True)\n",
    "\n",
    "# exceedance map directory\n",
    "exceed_path = arr_dir+'exceed_arrays/10yr/'\n",
    "os.makedirs(exceed_path,exist_ok=True)\n",
    "\n",
    "# exceedance point list directory\n",
    "exceedpt_path = arr_dir+'exceedpts/10yr/'\n",
    "os.makedirs(exceedpt_path,exist_ok=True)\n",
    "\n",
    "# event map figure export directory\n",
    "fig_path = arr_dir+'event_maps/'\n",
    "os.makedirs(fig_path,exist_ok=True)\n",
    "\n",
    "# log directory\n",
    "log_dir = datadir+'logs/'\n",
    "os.makedirs(log_dir,exist_ok=True)\n",
    "\n",
    "# 1h st4 files\n",
    "files = [f for f in os.listdir(path01) if f.endswith('.grb2')]\n",
    "\n",
    "# 6h st4 files\n",
    "files6 = [f for f in os.listdir(path06) if f.endswith('.grb2')]\n",
    "\n",
    "# 24h st4 files\n",
    "files24 = [f for f in os.listdir(path24) if f.endswith('.grb2')]\n",
    "\n",
    "# create list of all times (by hour) to analyze\n",
    "all_times = pd.Series(pd.date_range(start_time,end_time, freq='h'))\n",
    "\n",
    "starttime = list(all_times)[0]\n",
    "times = [t for t in all_times if ((t.month==starttime.month) & (t.year==starttime.year))]\n",
    "\n",
    "# database file name\n",
    "dbfname = 'ere_database_prelim_'+str(sep_dist)+'km.csv'\n",
    "\n",
    "# lat/lon bounds for domain of study (W,E,S,N)\n",
    "west,east,south,north = -104,-65,20,50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d9b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open shapefile for creating CONUS masks\n",
    "shapefile = gpd.read_file(datadir+'states_21basic/states.shp')\n",
    "states = shapefile.geometry.to_list()\n",
    "state_abbr = shapefile.STATE_ABBR.to_list()\n",
    "\n",
    "# check if point is in one of the states then return state abreviation, else None\n",
    "def in_us(lat, lon):\n",
    "    p = Point(lon, lat)\n",
    "    for i in range(1,50):\n",
    "        if states[i].contains(p):\n",
    "            return state_abbr[i]\n",
    "    return None\n",
    "    \n",
    "# delete unnecessary idx files produced by xarray\n",
    "def wipe_idx(path):\n",
    "    files = [file for file in os.listdir(path) if file.endswith('.idx')]\n",
    "    for file in files:\n",
    "        os.remove(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b94e8-998e-426b-9472-7b6e57f21c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for retrieving initial lat/lon fields\n",
    "ds = xr.open_dataset(path01+'st4_conus.'+times[0].strftime('%Y%m%d%H')+'.01h.grb2')\n",
    "lat = ds.latitude.data\n",
    "lon = ds.longitude.data - 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c676c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate NOAA Atlas 14 data (12-hr precipitation frequency) onto stage iv grid\n",
    "def atlas_regrid():\n",
    "    global atlas_dir\n",
    "    global latver\n",
    "    global lat\n",
    "    global lon\n",
    "    \n",
    "    # load noaa altas 14 dataset\n",
    "    ds_atlas = xr.open_dataset(atlas_dir+'NOAA_Atlas_14_CONUS.nc')\n",
    "    \n",
    "    alat = ds_atlas.lat.data\n",
    "    alon = ds_atlas.lon.data\n",
    "    \n",
    "    # get 12-hr data and convert inches to mm\n",
    "    atlas_pf12 = ds_atlas.pf_012_hr*25.4\n",
    "    ARIs = atlas_pf12.ari\n",
    "    \n",
    "    # export arrays for the following ARIs with pf_012_hr\n",
    "    ARI_selections = [10,25,50,100,500,1000]\n",
    "\n",
    "    for ARI in ARI_selections:\n",
    "        atlas_data = atlas_pf12[:,:,ARIs==ARI]\n",
    "\n",
    "        # replace nan with high value to prevent triggers at coastline\n",
    "        atlas_nonan = np.nan_to_num(atlas_data.data,nan=1e3)\n",
    "\n",
    "        # regrid/interpolate atlas to match stage-iv\n",
    "        f = interpolate.interp2d(alon,alat,atlas_nonan,kind='linear')\n",
    "\n",
    "        atlas_regrided = np.empty_like(lat)\n",
    "\n",
    "        for latix in range(atlas_regrided.shape[0]):\n",
    "            for lonix in range(atlas_regrided.shape[1]):\n",
    "                atlas_regrided[latix,lonix] = f(lon[latix,lonix],lat[latix,lonix])\n",
    "\n",
    "        np.save(atlas_dir+'NOAA_Atlas_14_CONUS_pf_012_hr_'+str(ARI)+'yr_regrid_'+str(latver)+'.npy',atlas_regrided)\n",
    "        \n",
    "        # clear RAM\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de0c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a location mask (within USA borders/coastlines) for the current lat/lon grid\n",
    "def us_mask(lat,lon):\n",
    "    global usmask_dir\n",
    "    global latver\n",
    "    \n",
    "    os.makedirs(usmask_dir,exist_ok=True)\n",
    "\n",
    "    binary = np.zeros(lat.shape)\n",
    "\n",
    "    for lt in range(lat.shape[0]):\n",
    "        for ln in range(lat.shape[1]):\n",
    "            # check if in US\n",
    "            if in_us(lat[lt,ln],lon[lt,ln]) != None:\n",
    "                binary[lt,ln] = 1\n",
    "                \n",
    "    # location mask\n",
    "    lmask = np.ma.make_mask(binary)\n",
    "    \n",
    "    np.save(usmask_dir+'US_mask_'+str(latver)+'.npy',lmask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2adba1c-99e8-49a0-b170-17bc135c58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export lat/lon grids if not already done for this lat/lon version... may take several minutes\n",
    "if not os.path.exists(latlon_dir+'stageiv_lats_'+str(latver)+'.npy'):\n",
    "    np.save(latlon_dir+'stageiv_lats_'+str(latver)+'.npy',lat)\n",
    "    np.save(latlon_dir+'stageiv_lons_'+str(latver)+'.npy',lon)\n",
    "    \n",
    "    # regrid atlas and us mask data\n",
    "    atlas_regrid()\n",
    "    us_mask(lat,lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4f015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create basic lat/lon masks for domain\n",
    "latmask = (lat>=south) & (lat<=north)\n",
    "lonmask = (lon>=west) & (lon<=east)\n",
    "llmask = latmask & lonmask\n",
    "\n",
    "# load regridded atlas arrays\n",
    "atlas = {}\n",
    "for ARI in [10,25,50,100,500,1000]:\n",
    "    atlas[str(ARI)] = np.load(atlas_dir+'NOAA_Atlas_14_CONUS_pf_012_hr_'+str(ARI)+'yr_regrid_'+str(latver)+'.npy')\n",
    "\n",
    "# load the mask file\n",
    "US_mask = np.load(usmask_dir+'US_mask_'+str(latver)+'.npy')\n",
    "\n",
    "# combine lat/lon and US masks\n",
    "mask = llmask.data & US_mask\n",
    "mask_i = np.invert(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8e1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subroutine to retrieve 12hr precip from 1hr data files\n",
    "# tp12 is the 12-hr accumulation array from the previous iteration\n",
    "# tstr is the formated time string for the current iteration\n",
    "def get_12hr_precip(tp12,tstr):\n",
    "    global lat\n",
    "    global lon\n",
    "    # for first iteration, fill array with all 12 hours\n",
    "    if np.nansum(tp12)==0:\n",
    "        # fill array with data and sum 12 hours\n",
    "        for idx,time12 in enumerate(times12):\n",
    "            # time in string format\n",
    "            tstr = time12.strftime('%Y%m%d%H')\n",
    "            # 01h filename\n",
    "            f = 'st4_conus.'+tstr+'.01h.grb2'\n",
    "            # make sure data is not missing\n",
    "            if os.path.exists(path01+f):\n",
    "                # open data and get 1h precip array\n",
    "                ds = xr.open_dataset(path01+f)\n",
    "                lat = ds.latitude.data\n",
    "                lon = ds.longitude.data - 360\n",
    "                if 'tp' in list(ds.variables):\n",
    "                    tp01 =  ds.tp.data\n",
    "                else:\n",
    "                    # fill with zeros\n",
    "                    tp01 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                    tp12[idx] = tp01\n",
    "                ds.close()\n",
    "                del(ds)\n",
    "                # clear RAM\n",
    "                gc.collect()\n",
    "                \n",
    "                tp12[idx] = tp01\n",
    "            else:\n",
    "                # fill with zeros\n",
    "                tp01 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                tp12[idx] = tp01\n",
    "\n",
    "        tp = np.nansum(tp12,axis=0)\n",
    "        \n",
    "    else:\n",
    "        # 01h filename\n",
    "        f = 'st4_conus.'+tstr+'.01h.grb2'\n",
    "        # make sure data is not missing\n",
    "        if os.path.exists(path01+f):\n",
    "            # open data and get 1h precip array\n",
    "            ds = xr.open_dataset(path01+f)\n",
    "            lat = ds.latitude.data\n",
    "            lon = ds.longitude.data - 360\n",
    "            if 'tp' in list(ds.variables):\n",
    "                tp01 =  ds.tp.data\n",
    "            else:\n",
    "                # fill with zeros\n",
    "                tp01 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                # append new hour of data to previous array excluding the first hour\n",
    "                tp12 = np.append(tp12[1:],np.array([tp01]),axis=0)\n",
    "                tp = np.nansum(tp12,axis=0)\n",
    "            ds.close()\n",
    "            del(ds)\n",
    "            # clear RAM\n",
    "            gc.collect()\n",
    "            \n",
    "            # append new hour of data to previous array excluding the first hour\n",
    "            tp12 = np.append(tp12[1:],np.array([tp01]),axis=0)\n",
    "            tp = np.nansum(tp12,axis=0)\n",
    "        else:\n",
    "            # fill with zeros\n",
    "            tp01 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "            # append new hour of data to previous array excluding the first hour\n",
    "            tp12 = np.append(tp12[1:],np.array([tp01]),axis=0)\n",
    "            tp = np.nansum(tp12,axis=0)\n",
    "        \n",
    "    return(tp12,tp,lat,lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6515dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to apply 6-hr data QC\n",
    "def cross_check_06():\n",
    "    global tp06sum\n",
    "    # if same times as previous iteration, keep previous accumulation, else create new\n",
    "    if not np.array_equal(times06,times06_old):\n",
    "        # new precip array (from 6hr data)\n",
    "        tp18 = np.empty((len(times06), lat.shape[0], lat.shape[1]))\n",
    "        # fill array with data and sum\n",
    "        for idx,time06 in enumerate(times06):\n",
    "            # time in string format\n",
    "            tstr = time06.strftime('%Y%m%d%H')\n",
    "            # 06h filename\n",
    "            f = 'st4_conus.'+tstr+'.06h.grb2'\n",
    "            # make sure data is not missing\n",
    "            if os.path.exists(path06+f):\n",
    "                # open data and get 6h precip array\n",
    "                ds = xr.open_dataset(path06+f)\n",
    "                if 'tp' in list(ds.variables):\n",
    "                    tp06 =  ds.tp.data\n",
    "                    tp18[idx] = tp06\n",
    "                else:\n",
    "                    # fill with zeros\n",
    "                    tp06 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                    tp18[idx] = tp06\n",
    "                ds.close()\n",
    "                del(ds)\n",
    "                # clear RAM\n",
    "                gc.collect()\n",
    "            else:\n",
    "                # fill with zeros\n",
    "                tp06 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                tp18[idx] = tp06\n",
    "            tp06sum = np.nansum(tp18,axis=0)\n",
    "        \n",
    "    tpdiff = accum_raw - tp06sum\n",
    "\n",
    "    # subtract difference to replace all erroneously high values with 6-hr data values\n",
    "    accum_qc06 = accum_raw - np.nan_to_num(tpdiff * (tpdiff > 0))\n",
    "    \n",
    "    return(accum_qc06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a214ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to apply 24-hr data QC on 6-hr QC'd data\n",
    "def cross_check_24():\n",
    "    global tp24sum\n",
    "    # if same times as previous iteration, keep previous accumulation, else create new\n",
    "    if not np.array_equal(times24,times24_old):\n",
    "        # new precip array (from 6hr data)\n",
    "        tp48 = np.empty((len(times24), lat.shape[0], lat.shape[1]))\n",
    "        # fill array with data and sum\n",
    "        for idx,time24 in enumerate(times24):\n",
    "            # time in string format\n",
    "            tstr = time24.strftime('%Y%m%d')+'12'\n",
    "            # 24h filename\n",
    "            f = 'st4_conus.'+tstr+'.24h.grb2'\n",
    "            # make sure data is not missing\n",
    "            if os.path.exists(path24+f):\n",
    "                # open data and get 6h precip array\n",
    "                ds = xr.open_dataset(path24+f)\n",
    "                if 'tp' in list(ds.variables):\n",
    "                    tp24 =  ds.tp.data\n",
    "                    tp48[idx] = tp24\n",
    "                else:\n",
    "                    # fill with zeros\n",
    "                    tp24 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                    tp48[idx] = tp24\n",
    "                ds.close()\n",
    "                del(ds)\n",
    "                # clear RAM\n",
    "                gc.collect()\n",
    "            else:\n",
    "                # fill with zeros\n",
    "                tp24 =  np.zeros((lat.shape[0], lat.shape[1]))\n",
    "                tp48[idx] = tp24\n",
    "            tp24sum = np.nansum(tp48,axis=0)\n",
    "        \n",
    "    tpdiff = accum_qc06 - tp24sum\n",
    "\n",
    "    # subtract difference to replace all erroneously high values with 24-hr data values\n",
    "    accum = accum_qc06 - np.nan_to_num(tpdiff * (tpdiff > 0))\n",
    "    \n",
    "    return(accum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437b1da-676d-4c56-90ef-7e37269d3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_member(a, b):\n",
    "    a_set = set(a)\n",
    "    b_set = set(b)\n",
    " \n",
    "    if (a_set & b_set):\n",
    "        common = list(a_set & b_set)\n",
    "    else:\n",
    "        common = []\n",
    "        \n",
    "    return(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccc9dfb-91e4-4b10-a887-1ece0f819e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sorted data for any events with shared exceedance points within accumulation window\n",
    "# this happens when hourly events at the same location are not continuous (i.e. events occur more than once within accumulation period)\n",
    "# merge those events and export new sorted data\n",
    "def merge_repeats():\n",
    "    # merged events database\n",
    "    data = {}\n",
    "    for col in cols[1:]:\n",
    "        data[col] = []\n",
    "    # array data\n",
    "    adata = {}\n",
    "    for arr in arrs:\n",
    "        adata[arr] = []\n",
    "        \n",
    "    # list of indexes combined\n",
    "    ematchidxall = []\n",
    "    \n",
    "    for i in range(len(data_sorted['lat'])):\n",
    "        # if already been combined, skip\n",
    "        if i in ematchidxall:\n",
    "            continue\n",
    "    \n",
    "        # initialize list of matching event indexes\n",
    "        ematchidxs = [i]\n",
    "    \n",
    "        # initialize list of exceedance points for event\n",
    "        expts = arrays_sorted['exceed_points'][i].copy()\n",
    "        \n",
    "        # indexes of events to check for shared accumulation period\n",
    "        check_next = 10\n",
    "        # can increase this value if dealing with a very large number of events in a short timespan\n",
    "        \n",
    "        for i2 in [a for a in range(i+1,i+check_next+1) if ((a < len(data_sorted['lat'])) & (a not in ematchidxall))]:\n",
    "            # check for shared exceedance points or max exceedance points within sep_dist\n",
    "            if (data_sorted['period_start'][i2] < data_sorted['period_end'][i]) & (data_sorted['period_end'][i2] > data_sorted['period_start'][i]):\n",
    "                commonpts = common_member(expts, arrays_sorted['exceed_points'][i2])\n",
    "\n",
    "                # if events occur within 12 hours of each other\n",
    "                event_time_condition = abs((data_sorted['event_time'][i] - data_sorted['event_time'][i2]).total_seconds()/3600) < 12\n",
    "\n",
    "                # if max exceedance points are within sep_dist\n",
    "                pt1 = (data_sorted['lat'][i], data_sorted['lon'][i])\n",
    "                pt2 = (data_sorted['lat'][i2], data_sorted['lon'][i2])\n",
    "                sep_dist_condition = geodesic(pt1, pt2).km < sep_dist\n",
    "                \n",
    "                if (len(commonpts) > 0) | (event_time_condition & sep_dist_condition):\n",
    "                    # save index of event\n",
    "                    ematchidxs.append(i2)\n",
    "                    # add new exceedance points\n",
    "                    expts += list(set(arrays_sorted['exceed_points'][i2]) - set(expts))\n",
    "                    \n",
    "        # if any matches, combine and save to new events database\n",
    "        if len(ematchidxs) > 1:\n",
    "            ematchidxall+=ematchidxs\n",
    "            \n",
    "            # combine accumulation arrays to save only max values at each gridpoint\n",
    "            matchaccums = [np.nan_to_num(arrays_sorted['accum_arrays'][i]) for i in ematchidxs]\n",
    "            accum_combined = np.maximum.reduce(matchaccums)\n",
    "            \n",
    "            # accumulation period start and end times\n",
    "            period_start = np.nanmin([data_sorted['period_start'][i] for i in ematchidxs])\n",
    "            period_end = np.nanmax([data_sorted['period_end'][i] for i in ematchidxs])\n",
    "            \n",
    "            # find idx with max exceedance\n",
    "            matchexceeds = [data_sorted['exceedance'][i] for i in ematchidxs]\n",
    "            emaxidx = ematchidxs[matchexceeds.index(max(matchexceeds))]\n",
    "            \n",
    "            # create exceedance arrays\n",
    "            # mask out all points not included in this event (in case of simulataneous events)    \n",
    "            exmask = (lat==expts[0][0]) & (lon==expts[0][1])\n",
    "            for expt in expts[1:]:\n",
    "                exmask += (lat==expt[0]) & (lon==expt[1])\n",
    "            atlas_e = np.load(atlas_dir+'NOAA_Atlas_14_CONUS_pf_012_hr_10yr_regrid_'+str(int(data_sorted['latversion'][emaxidx]))+'.npy')\n",
    "            exceed_array = exmask*accum_combined-atlas_e*exmask\n",
    "            \n",
    "            # calculate exceedance volume\n",
    "            exvolume = np.nansum(exceed_array)\n",
    "            \n",
    "            # record data for event\n",
    "            data['lat'].append(data_sorted['lat'][emaxidx])\n",
    "            data['lon'].append(data_sorted['lon'][emaxidx])\n",
    "            data['state'].append(data_sorted['state'][emaxidx])\n",
    "            data['event_time'].append(data_sorted['event_time'][emaxidx])\n",
    "            data['start_time'].append(data_sorted['start_time'][emaxidx])\n",
    "            data['accum_time'].append(data_sorted['accum_time'][emaxidx])\n",
    "            data['duration'].append(data_sorted['duration'][emaxidx])\n",
    "            data['period_start'].append(period_start)\n",
    "            data['period_end'].append(period_end)\n",
    "            data['exceedance'].append(data_sorted['exceedance'][emaxidx])\n",
    "            data['accumulation'].append(data_sorted['accumulation'][emaxidx])\n",
    "            data['exceed_pts'].append(len(expts))\n",
    "            data['exceed_vol'].append(exvolume)\n",
    "            data['max_1hr'].append(data_sorted['max_1hr'][emaxidx])\n",
    "            data['ari_10'].append(data_sorted['ari_10'][emaxidx])\n",
    "            data['ari_25'].append(data_sorted['ari_25'][emaxidx])\n",
    "            data['ari_50'].append(data_sorted['ari_50'][emaxidx])\n",
    "            data['ari_100'].append(data_sorted['ari_100'][emaxidx])\n",
    "            data['ari_500'].append(data_sorted['ari_500'][emaxidx])\n",
    "            data['ari_1000'].append(data_sorted['ari_1000'][emaxidx])\n",
    "            data['latversion'].append(data_sorted['latversion'][emaxidx])\n",
    "            \n",
    "            # append arrays to lists\n",
    "            adata['accum_arrays'].append(accum_combined)\n",
    "            adata['exceed_arrays'].append(exceed_array)\n",
    "            adata['exceed_points'].append(expts)\n",
    "\n",
    "    # delete events that were combined and append merged event(s)\n",
    "    for col in cols[1:]:\n",
    "        for i in sorted(ematchidxall, reverse=True):\n",
    "            del data_sorted[col][i]\n",
    "        data_sorted[col] += data[col]\n",
    "    \n",
    "    for arr in arrs:\n",
    "        for i in sorted(ematchidxall, reverse=True):\n",
    "            del arrays_sorted[arr][i]\n",
    "        arrays_sorted[arr] += adata[arr]\n",
    "\n",
    "    # re-sort lists\n",
    "    # sort each list by event time\n",
    "    sortedIndex = sorted(range(len(data_sorted['event_time'])), key=lambda k: data_sorted['event_time'][k])\n",
    "    data_sorted2 = {}\n",
    "    for col in cols[1:]:\n",
    "        data_sorted2[col] = [data_sorted[col][i] for i in sortedIndex]\n",
    "    \n",
    "    # for array lists\n",
    "    arrays_sorted2 = {}\n",
    "    for arr in arrs:\n",
    "        arrays_sorted2[arr] = [arrays_sorted[arr][i] for i in sortedIndex]\n",
    "\n",
    "    return(data_sorted2,arrays_sorted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe0bb0-20d6-491c-aa11-829d409aa395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate and export accumulation maps for each month\n",
    "def generate_figures():\n",
    "    # create figure\n",
    "    fig = plt.figure(figsize=(8,5),dpi=300)\n",
    "    \n",
    "    events = data_sorted['event']\n",
    "    for i in range(len(data_sorted['event'])):\n",
    "        event = data_sorted['event'][i]\n",
    "        print('Generating figures: '+str(event)+'/'+str(events[-1]),end='\\r')\n",
    "        accum = arrays_sorted['accum_arrays'][i]\n",
    "        expts = arrays_sorted['exceed_points'][i]\n",
    "        thresh = data_sorted['ari_10'][i]\n",
    "        maxlat = data_sorted['lat'][i]\n",
    "        maxlon = data_sorted['lon'][i]\n",
    "        period_start = data_sorted['period_start'][i]\n",
    "        period_end = data_sorted['period_end'][i]\n",
    "        \n",
    "        exlats = [l[0] for l in expts]\n",
    "        exlons = [l[1] for l in expts]\n",
    "        \n",
    "        # create geo-axis\n",
    "        ax = fig.add_subplot(111, projection=ccrs.PlateCarree())\n",
    "        \n",
    "        # plot the data\n",
    "        pcm = plt.pcolormesh(lon,lat,accum, cmap='cubehelix_r', transform=ccrs.PlateCarree(), \n",
    "                             vmin=0, vmax=thresh)\n",
    "        \n",
    "        plt.scatter(exlons,exlats,marker='o',facecolors='none',edgecolors='r',s=1.5,linewidths=0.4,\n",
    "                    transform=ccrs.PlateCarree(),zorder=3)\n",
    "        \n",
    "        plt.title(str(event)+'   '+str(period_start)+' â€“ '+str(period_end),loc='left')\n",
    "        \n",
    "        cbar = plt.colorbar(pcm,orientation='vertical',extend='max',pad=0.02)\n",
    "        cbar.set_label(label='12-hr QPE (mm)')\n",
    "        \n",
    "        # add geography\n",
    "        ax.coastlines(lw=0.5)\n",
    "        ax.add_feature(cfeature.BORDERS,lw=0.5)\n",
    "        ax.add_feature(cfeature.STATES,lw=0.5)\n",
    "        \n",
    "        extent = [min([(maxlon-4.5),(np.nanmin(exlons)-0.5)]),max([(maxlon+4.5),(np.nanmax(exlons)+0.5)]),\n",
    "                  min([(maxlat-4.5),(np.nanmin(exlats)-0.5)]),max([(maxlat+4.5),(np.nanmax(exlats)+0.5)])]\n",
    "        \n",
    "        ax.set_extent(extent)\n",
    "        \n",
    "        gd = Geodesic()\n",
    "        cp250 = gd.circle(lon=maxlon, lat=maxlat, radius=sep_dist*1000)\n",
    "        geom = [sgeom.Polygon(cp250)]\n",
    "        \n",
    "        ax.add_geometries(geom, crs=ccrs.PlateCarree(), facecolor='none', edgecolor='k', alpha=0.5, linewidth = 0.5)\n",
    "        \n",
    "        ax.scatter([maxlon],[maxlat],marker='o',facecolors='none',edgecolors='white',s=1.5,linewidths=0.6,\n",
    "                    transform=ccrs.PlateCarree(),zorder=3)\n",
    "            \n",
    "        plt.savefig(fig_path+str(event).zfill(5)+'.jpg', dpi=300, bbox_inches='tight')\n",
    "\n",
    "        # clear figure contents so figure can be reused (saves RAM)\n",
    "        plt.clf()\n",
    "        \n",
    "    plt.close()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b935c8b1-8ee3-420b-8e64-55c29f906c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of column names to record data\n",
    "cols = ['event', 'lat', 'lon', 'state', 'event_time', 'start_time', 'accum_time', 'duration', 'period_start', 'period_end', \n",
    "        'exceedance', 'accumulation', 'exceed_pts', 'exceed_vol', 'max_1hr', 'ari_10', 'ari_25', 'ari_50', 'ari_100',\n",
    "        'ari_500', 'ari_1000', 'latversion']\n",
    "        \n",
    "# PARAMETER DESCRIPTIONS\n",
    "# 'event': event ID\n",
    "# 'lat', 'lon', 'state': # lats/lons/state of point of max exceedance\n",
    "# 'event_time': time with max hourly total (at point of max exceedance)\n",
    "# 'start_time': start time of hourly precip >=1 mm at point of max exceedance\n",
    "# 'accum_time': end time of 12-hr accumulation period at point of max exceedance\n",
    "# 'duration': difference between start and accum time\n",
    "# 'period_start': full accumulation period start time (first merged event accum_time -12 hrs)\n",
    "# 'period_end': full accumulation period end times (last merged event accum_time)\n",
    "# 'exceedance': max exceedance above the 10-yr ARI threshold\n",
    "# 'accumulation': 12-hr accumulation value at point of max exceedance\n",
    "# 'exceed_pts': total # of gridpoints exceeding 10-yr ARI threshold\n",
    "# 'exceed_vol': sum of 10-yr ARI exceedance at all gridpoints in event\n",
    "# 'max_1hr': maximum 1-hr rainfall at point of max exceedance\n",
    "# 'ari_10', 'ari_25', 'ari_50', 'ari_100', 'ari_500', 'ari_1000': ARI thresholds at point of max exceedance for respective return periods\n",
    "# 'latversion': lat/lon grid version ID (changes a few times through the stage iv dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c10d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAREFUL: WILL REPACE PREVIOUS DATA. INITIALIZATION ONLY! (make sure resume = True if resuming)\n",
    "# Creates new csv file with empty columns to store data\n",
    "if not resume:\n",
    "    # create dictionary of empty lists to store data\n",
    "    data = {}\n",
    "    for col in cols:\n",
    "        data[col] = []\n",
    "\n",
    "    # create dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # SAVE TO NEW FILE\n",
    "    df.to_csv(datadir+dbfname,index=False)\n",
    "    \n",
    "    # arrays to save for each event\n",
    "    # merged/max 12-hr accumulation arrays (2d maps)\n",
    "    accum_arrays = []\n",
    "    # 2d maps of exceedance above 10-yr ARI thresholds\n",
    "    exceed_arrays = []\n",
    "    # lists of exceedance points\n",
    "    exceed_points = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb3cae9-7c46-47f4-8230-97c11633e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "#### Execute ####\n",
    "##############################################################################################\n",
    "code_start = timemod.time()\n",
    "code_start_str = timemod.strftime(\"%Y%m%d%H%M%S\", timemod.localtime())\n",
    "\n",
    "current_time = timemod.strftime(\"%Y-%m-%d %H:%M:%S\", timemod.localtime())\n",
    "\n",
    "# create log file\n",
    "with open(log_dir+'log_'+str(sep_dist)+'km_'+code_start_str+'.txt', 'w') as f:\n",
    "    f.write('Start: '+current_time+'\\n')\n",
    "\n",
    "wipe_idx(path01)\n",
    "wipe_idx(path06)\n",
    "wipe_idx(path24)\n",
    "\n",
    "# execute algorithm one month at a time until all times have been analyzed\n",
    "while len(all_times)>0:\n",
    "    elapsed_time = timemod.time() - code_start\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    time_elapsed = f\"Elapsed time: {int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n",
    "    print(time_elapsed)\n",
    "    \n",
    "    # dataframe up to previous month\n",
    "    df0 = pd.read_csv(datadir+dbfname)\n",
    "    \n",
    "    # Convert strings to datetime\n",
    "    if len(df0)>0:\n",
    "        df0['event_time'] = pd.to_datetime(df0['event_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df0['accum_time'] = pd.to_datetime(df0['accum_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df0['period_start'] = pd.to_datetime(df0['period_start'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        df0['period_end'] = pd.to_datetime(df0['period_end'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        # cut off previous database at start of period\n",
    "        df0 = df0[df0['period_end']<all_times[0]]\n",
    "\n",
    "        # Initialize event ID number\n",
    "        event_id = df0['event'].to_list()[-1]+1\n",
    "\n",
    "    else:\n",
    "        event_id = 1\n",
    "    \n",
    "    # define lists for hourly preliminary events\n",
    "    expts_h,maxpt_h,event_time_h,event_max_h,event_exceedance_h,maxrate_h,event_size_h,\\\n",
    "    event_state_h,start_time_h,accum_time_h,accum_array_h,latver_h = [],[],[],[],[],[],[],[],[],[],[],[]\n",
    "    \n",
    "    # define empty lists to store data (see above cell for list of columns and descriptions)\n",
    "    data = {}\n",
    "    for col in cols:\n",
    "        data[col] = []\n",
    "        \n",
    "    # arrays to save for each event\n",
    "    # merged/max 12-hr accumulation arrays (2d maps)\n",
    "    accum_arrays = []\n",
    "    # 2d maps of exceedance above 10-yr ARI thresholds\n",
    "    exceed_arrays = []\n",
    "    # lists of exceedance points\n",
    "    exceed_points = []\n",
    "    \n",
    "    # initialize cross-check file times\n",
    "    times06_old = 0\n",
    "    times24_old = 0\n",
    "    \n",
    "    # 12hr precip array\n",
    "    tp12 = np.empty((12, lat.shape[0], lat.shape[1]))\n",
    "    \n",
    "    # obtain list of timestamps for month to iterate through\n",
    "    mstarttime = list(all_times)[0]\n",
    "    times = [t for t in all_times if ((t.month==mstarttime.month) & (t.year==mstarttime.year))]\n",
    "    \n",
    "    # loop for each hour in the month\n",
    "    for index,time in enumerate(times):\n",
    "        # show progress percentage for the month\n",
    "        print(str(time)+' '+\"{:.2f}\".format((index+1)/len(times)*100)+'% ',end='\\r')\n",
    "        \n",
    "        # time in string format\n",
    "        tstr = time.strftime('%Y%m%d%H')\n",
    "        \n",
    "        # list of times in accumulation period\n",
    "        times12 = pd.Series(pd.date_range(time-pd.Timedelta(11,'h'),time, freq='h'))\n",
    "        \n",
    "        # list of times for 6hr data files -- up to 3 (note that file names are the END times of 6h periods)\n",
    "        times06 = pd.Series(pd.date_range((time-pd.Timedelta(6,'h')).floor('6h'),\n",
    "                                          (time+pd.Timedelta(5,'h')).floor('6h'), freq='6h'))\n",
    "        # same for 24hr data files -- up to 2 (note that 24hr files run from 12z to 12z each day)\n",
    "        times24 = pd.Series(pd.date_range(time.floor('24h'),(time+pd.Timedelta(11,'h')).floor('24h')))\n",
    "    \n",
    "        # previous latitude array\n",
    "        lat_prev = lat.copy()\n",
    "    \n",
    "        # get data\n",
    "        tp12,accum_raw,lat,lon = get_12hr_precip(tp12,tstr)\n",
    "    \n",
    "        # save new lat/lon arrays if lat array is different from previous\n",
    "        if np.array_equal(lat,lat_prev) == False:\n",
    "            latver+=1\n",
    "            if not os.path.exists(latlon_dir+'stageiv_lats_'+str(latver)+'.npy'):\n",
    "                print('\\nGenerating new lat/lon version grids')\n",
    "                np.save(latlon_dir+'stageiv_lats_'+str(latver)+'.npy',lat)\n",
    "                np.save(latlon_dir+'stageiv_lons_'+str(latver)+'.npy',lon)\n",
    "                # redo usmask\n",
    "                us_mask(lat,lon)\n",
    "                # regrid atlas data\n",
    "                atlas_regrid()\n",
    "            \n",
    "            latmask = (lat>=south) & (lat<=north)\n",
    "            lonmask = (lon>=west) & (lon<=east)\n",
    "            llmask = latmask & lonmask\n",
    "            \n",
    "            US_mask = np.load(usmask_dir+'US_mask_'+str(latver)+'.npy')\n",
    "            mask = llmask.data & US_mask\n",
    "            mask_i = np.invert(mask)\n",
    "            \n",
    "            # create dictionary of atlas arrays\n",
    "            atlas = {}\n",
    "            for ARI in [10,25,50,100,500,1000]:\n",
    "                atlas[str(ARI)] = np.load(atlas_dir+'NOAA_Atlas_14_CONUS_pf_012_hr_'+str(ARI)+'yr_regrid_'+str(latver)+'.npy')\n",
    "            \n",
    "        # perform cross-checks on data to take advantage of manual QC on 6 and 24 hr data\n",
    "        accum_qc06 = cross_check_06()\n",
    "        accum = cross_check_24()        \n",
    "        \n",
    "        # save times for next iteration to avoid unnecessary repeated opening of files\n",
    "        times06_old = times06.copy()\n",
    "        times24_old = times24.copy()\n",
    "    \n",
    "        # apply mask\n",
    "        accum[mask_i] = np.nan\n",
    "    \n",
    "        # subtract threshold map\n",
    "        exceed = accum-atlas['10']\n",
    "    \n",
    "        # if any points exceeding threshold\n",
    "        if np.nanmax(exceed) > 0:\n",
    "            # pull out points exceeding threshold\n",
    "            extreme_mask = exceed>0\n",
    "            exlats = lat[extreme_mask]\n",
    "            exlons = lon[extreme_mask]\n",
    "            exvals = accum[extreme_mask]\n",
    "            exexceed = exceed[extreme_mask]\n",
    "    \n",
    "            # check for simultaneous events within sep_dist km\n",
    "            # list index of points in separate simultaneous events (all points for first iteration)\n",
    "            separate = list(range(len(exvals)))\n",
    "    \n",
    "            # save original expoints\n",
    "            exlats_orig = exlats.copy()\n",
    "            exlons_orig = exlons.copy()\n",
    "            expts_orig = [(exlats[i], exlons[i]) for i in range(len(exlats))]\n",
    "    \n",
    "            # while points not sampled yet, count additional events\n",
    "            while len(separate) > 0:\n",
    "                # index points that have not yet been sampled\n",
    "                exlats = exlats[separate]\n",
    "                exlons = exlons[separate]\n",
    "                exvals = exvals[separate]\n",
    "                exexceed = exexceed[separate]\n",
    "    \n",
    "                # index of max exceedance point\n",
    "                imax = np.nanargmax(exexceed)\n",
    "    \n",
    "                # max value\n",
    "                maxtp = exvals[imax]\n",
    "                maxexceed = exexceed[imax]\n",
    "    \n",
    "                # lat/lon of max value\n",
    "                maxlat = exlats[imax]\n",
    "                maxlon = exlons[imax]\n",
    "    \n",
    "                # coordinate of max value\n",
    "                maxcoord = (maxlat, maxlon)\n",
    "    \n",
    "                # list index of points in separate simultaneous events\n",
    "                separate = []\n",
    "    \n",
    "                # list exceedance point lat/lons\n",
    "                exceedpts = []\n",
    "    \n",
    "                 # check each extreme point and save index for next iteration if outside sep_dist\n",
    "                for ptidx in range(len(exvals)):\n",
    "                    pt = (exlats[ptidx], exlons[ptidx])\n",
    "                    dist = geodesic(maxcoord, pt).km\n",
    "                    if dist > sep_dist:\n",
    "                        separate.append(ptidx)\n",
    "                    \n",
    "                # save all exceedpts within sep_dist\n",
    "                # include expoints already sampled so nearby events are combined\n",
    "                for ptidx in range(len(exlats_orig)):\n",
    "                    pt = (exlats_orig[ptidx], exlons_orig[ptidx])\n",
    "                    dist = geodesic(maxcoord, pt).km\n",
    "                    if dist <= sep_dist:\n",
    "                        exceedpts.append(pt)\n",
    "    \n",
    "                numpoints = len(exceedpts)\n",
    "    \n",
    "                # check previous 12 hours for time of max rain rate at point of max exceedance\n",
    "                accum_1hr = []\n",
    "                hour_idx = []\n",
    "                for hidx in range(12):\n",
    "                    accum_1hr.append(tp12[hidx][(lat==maxlat) & (lon==maxlon)][0])\n",
    "                maxrate = np.nanmax(accum_1hr)\n",
    "                idx_max_hour = accum_1hr.index(np.nanmax(accum_1hr))\n",
    "                # fist hour within accum_period-hr window where >=1 mm of rain accumulates at point of max exceedance\n",
    "                idx_start = accum_1hr.index(next(i for i in accum_1hr if i >= 1))\n",
    "                # time of max precip rate (occurring within the *following* hour)\n",
    "                maxtime = times12[idx_max_hour]-pd.Timedelta(1,'h')\n",
    "                starttime = times12[idx_start]-pd.Timedelta(1,'h')\n",
    "    \n",
    "                # record data as new preliminary event\n",
    "                expts_h.append(exceedpts)\n",
    "                maxpt_h.append((maxlat,maxlon))\n",
    "                event_time_h.append(maxtime)\n",
    "                event_max_h.append(maxtp)\n",
    "                event_exceedance_h.append(maxexceed)\n",
    "                event_size_h.append(numpoints)\n",
    "                event_state_h.append(in_us(maxlat,maxlon))\n",
    "                start_time_h.append(starttime)\n",
    "                accum_time_h.append(time)\n",
    "                maxrate_h.append(maxrate)\n",
    "                accum_array_h.append(accum)\n",
    "                latver_h.append(latver)\n",
    "\n",
    "            # exit loop if RAM exceeds 97% and will resume from this point in the month\n",
    "            ram_percent = psutil.virtual_memory().percent\n",
    "            if ram_percent > 97:\n",
    "                times = [t for t in times if (t<=time)]\n",
    "                print('\\nRAM Full')\n",
    "                break\n",
    "\n",
    "    # merge hourly events with shared exceedance points within a 12-hr period\n",
    "    print('\\nMerging hourly events')\n",
    "    total = len(maxpt_h)\n",
    "    while len(maxpt_h)>0:\n",
    "        # always start with first value in lists\n",
    "        i=0\n",
    "        \n",
    "        # initialize list of event indexes that have matching points\n",
    "        ematchidxs = [i]\n",
    "        \n",
    "        # indexes of events to check\n",
    "        check_next = 500\n",
    "        # can increase this value if dealing with a very large number of events in a short timespan\n",
    "        irange = list(np.arange(i,i+check_next+1)[np.where(np.arange(i,i+check_next+1)<len(event_time_h))[0]])\n",
    "        \n",
    "        # initialize list of exceedance points for event\n",
    "        expts = expts_h[i].copy()\n",
    "        \n",
    "        # initialize number of additional matching events\n",
    "        addition = True\n",
    "        \n",
    "        while addition:\n",
    "            addition = False\n",
    "            # remove indexes already found\n",
    "            irange = list(set(irange) - set(ematchidxs))\n",
    "    \n",
    "            if len(irange) > 0:\n",
    "                # initialize i as first element in irange\n",
    "                i=irange[0]\n",
    "                for i2 in irange:        \n",
    "                    # time difference between accumulation times\n",
    "                    tdif = np.abs((accum_time_h[i] - accum_time_h[i2]) / np.timedelta64(1, 'h'))\n",
    "                    tcheck = tdif < 12\n",
    "            \n",
    "                    # if within 12 hours check if matching exceedance points\n",
    "                    if tcheck:\n",
    "                        # check for common exceedance points between any match from current or previous accumulation period\n",
    "                        atimeidxs = [e for e in ematchidxs if (np.abs((accum_time_h[i2] - accum_time_h[e]) / np.timedelta64(1, 'h')) <=1)]\n",
    "                        expts_atime = sum([expts_h[a] for a in atimeidxs], [])\n",
    "                        commonpts = common_member(expts_atime, expts_h[i2])\n",
    "            \n",
    "                        if len(commonpts) > 0:\n",
    "                            # save index of event\n",
    "                            ematchidxs.append(i2)\n",
    "                            \n",
    "                            # add new exceedance points\n",
    "                            expts += list(set(expts_h[i2]) - set(expts))\n",
    "            \n",
    "                            # change i to latest match in order to continue adding events within 12 hours\n",
    "                            i = i2\n",
    "                            \n",
    "                            addition = True\n",
    "        \n",
    "        # combine accumulation arrays to save only max values at each gridpoint\n",
    "        matchaccums = [np.nan_to_num(accum_array_h[i]) for i in ematchidxs]\n",
    "        accum_combined = np.maximum.reduce(matchaccums)\n",
    "        \n",
    "        # accumulation period start and end times\n",
    "        period_start = np.nanmin([accum_time_h[i] for i in ematchidxs])-pd.Timedelta(12,'h')\n",
    "        period_end = np.nanmax([accum_time_h[i] for i in ematchidxs])\n",
    "        \n",
    "        # find idx with max exceedance\n",
    "        matchexceeds = [event_exceedance_h[i] for i in ematchidxs]\n",
    "        emaxidx = ematchidxs[matchexceeds.index(max(matchexceeds))]\n",
    "    \n",
    "        event_dur = (accum_time_h[emaxidx] - start_time_h[emaxidx]) / np.timedelta64(1, 'h')\n",
    "    \n",
    "        maxlat,maxlon = maxpt_h[emaxidx]\n",
    "\n",
    "        # redefine lat/lon grids for event in case of different latversion\n",
    "        latver_e = int(latver_h[emaxidx])\n",
    "        lat_e = np.load(latlon_dir+'stageiv_lats_'+str(latver_e)+'.npy')\n",
    "        lon_e = np.load(latlon_dir+'stageiv_lons_'+str(latver_e)+'.npy')\n",
    "        \n",
    "        # get thresholds at point of max exceedance\n",
    "        # bug fix: sometimes time of max exceedance uses a different latver if it changed during the event period\n",
    "        ptidx = np.where((lat_e >= maxlat-1e-6) & (lat_e <= maxlat+1e-6) & (lon_e >= maxlon-1e-6) & (lon_e <= maxlon+1e-6))\n",
    "        if len(ptidx[0])==0:\n",
    "            # try each latver starting at latest\n",
    "            for latver_except in range(latver,0,-1):\n",
    "                lat_e = np.load(latlon_dir+'stageiv_lats_'+str(latver_except)+'.npy')\n",
    "                lon_e = np.load(latlon_dir+'stageiv_lons_'+str(latver_except)+'.npy')\n",
    "                ptidx = np.where((lat_e >= maxlat-1e-6) & (lat_e <= maxlat+1e-6) & (lon_e >= maxlon-1e-6) & (lon_e <= maxlon+1e-6))\n",
    "                if len(ptidx[0]) > 0:\n",
    "                    latver_e = latver_except\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "            \n",
    "        thresh = {}\n",
    "        for ARI in [10,25,50,100,500,1000]:\n",
    "            atlas_e = np.load(atlas_dir+'NOAA_Atlas_14_CONUS_pf_012_hr_'+str(ARI)+'yr_regrid_'+str(int(latver_e))+'.npy')\n",
    "            thresh[str(ARI)] = atlas_e[ptidx][0]\n",
    "        \n",
    "        # create exceedance arrays\n",
    "        # mask out all points not included in this event (in case of simulataneous events)    \n",
    "        exmask = (lat_e==expts[0][0]) & (lon_e==expts[0][1])\n",
    "        for expt in expts[1:]:\n",
    "            exmask += (lat_e==expt[0]) & (lon_e==expt[1])\n",
    "        atlas_e = np.load(atlas_dir+'NOAA_Atlas_14_CONUS_pf_012_hr_10yr_regrid_'+str(int(latver_e))+'.npy')\n",
    "        exceed_array = exmask*accum_combined-atlas_e*exmask\n",
    "        \n",
    "        # calculate exceedance volume\n",
    "        exvolume = np.nansum(exceed_array)\n",
    "        \n",
    "        # record data for event\n",
    "        data['lat'].append(maxlat)\n",
    "        data['lon'].append(maxlon)\n",
    "        data['state'].append(in_us(maxlat,maxlon))\n",
    "        data['event_time'].append(event_time_h[emaxidx])\n",
    "        data['start_time'].append(start_time_h[emaxidx])\n",
    "        data['accum_time'].append(accum_time_h[emaxidx])\n",
    "        data['duration'].append(event_dur)\n",
    "        data['period_start'].append(period_start)\n",
    "        data['period_end'].append(period_end)\n",
    "        data['exceedance'].append(event_exceedance_h[emaxidx])\n",
    "        data['accumulation'].append(event_max_h[emaxidx])\n",
    "        data['exceed_pts'].append(len(expts))\n",
    "        data['exceed_vol'].append(exvolume)\n",
    "        data['max_1hr'].append(maxrate_h[emaxidx])\n",
    "        data['ari_10'].append(thresh['10'])\n",
    "        data['ari_25'].append(thresh['25'])\n",
    "        data['ari_50'].append(thresh['50'])\n",
    "        data['ari_100'].append(thresh['100'])\n",
    "        data['ari_500'].append(thresh['500'])\n",
    "        data['ari_1000'].append(thresh['1000'])\n",
    "        data['latversion'].append(latver_e)\n",
    "        \n",
    "        # append arrays to lists\n",
    "        accum_arrays.append(accum_combined)\n",
    "        exceed_arrays.append(exceed_array)\n",
    "        exceed_points.append(expts)\n",
    "        \n",
    "        # remove indexes from all lists with suffix _h\n",
    "        lsts = [expts_h,maxpt_h,event_time_h,event_max_h,event_exceedance_h,maxrate_h,event_size_h,event_state_h,\n",
    "                start_time_h,accum_time_h,accum_array_h]\n",
    "        \n",
    "        for lst in lsts:\n",
    "            for i in sorted(ematchidxs, reverse=True):\n",
    "                del lst[i]\n",
    "\n",
    "    # sort each list by event time\n",
    "    sortedIndex = sorted(range(len(data['event_time'])), key=lambda k: data['event_time'][k])\n",
    "    data_sorted = {}\n",
    "    for col in cols[1:]:\n",
    "        data_sorted[col] = [data[col][i] for i in sortedIndex]\n",
    "    \n",
    "    # for array lists\n",
    "    arrs = ['accum_arrays','exceed_arrays','exceed_points']\n",
    "    arrays_sorted = {}\n",
    "    for arr in arrs:\n",
    "        arrays_sorted[arr] = [eval(arr)[i] for i in sortedIndex]\n",
    "\n",
    "    # append data from any events from previous period with end periods within 12 hours of the start of the period being analyzed (for merging)\n",
    "    if len(df0)>0:\n",
    "        df1 = df0[df0['period_end']>=times[0]-pd.Timedelta(12,'h')]\n",
    "        if len(df1)>0:\n",
    "            df1 = df0[df0['event']>=np.nanmin(df1['event'])]\n",
    "            df0.drop(df1.index, inplace=True)\n",
    "            event_id = list(df1['event'])[0]\n",
    "\n",
    "        for col in cols[1:]:\n",
    "            data_sorted[col] = list(df1[col]) + data_sorted[col]\n",
    "\n",
    "        arrays_prev = {}\n",
    "        for arr in arrs:\n",
    "            arrays_prev[arr] = []\n",
    "        for e in df1['event']:\n",
    "            arrays_prev['accum_arrays'].append(np.load(accum_path+str(e).zfill(5)+'.npy'))\n",
    "            arrays_prev['exceed_arrays'].append(np.load(exceed_path+str(e).zfill(5)+'.npy'))\n",
    "            arrays_prev['exceed_points'].append([tuple(p) for p in list(np.load(exceedpt_path+str(e).zfill(5)+'.npy'))])\n",
    "        for arr in arrs:\n",
    "            arrays_sorted[arr] = arrays_prev[arr] + arrays_sorted[arr]\n",
    "    \n",
    "    # merge repeated events\n",
    "    data_sorted,arrays_sorted = merge_repeats()\n",
    "\n",
    "    # record event ids\n",
    "    eids = range(event_id,event_id+len(data_sorted['lat']))\n",
    "    data_sorted['event'] = eids\n",
    "\n",
    "    print('Saving data')\n",
    "    # export all arrays for the month\n",
    "    for i,amap,emap,expt in zip(eids,arrays_sorted['accum_arrays'],arrays_sorted['exceed_arrays'],arrays_sorted['exceed_points']):\n",
    "        np.save(accum_path+str(i).zfill(5)+'.npy',amap)\n",
    "        np.save(exceed_path+str(i).zfill(5)+'.npy',emap)\n",
    "        np.save(exceedpt_path+str(i).zfill(5)+'.npy',np.array(expt))\n",
    "    \n",
    "    # create dataframe\n",
    "    df2 = pd.DataFrame(data_sorted)\n",
    "    \n",
    "    # move event column to front\n",
    "    df2 = df2[ ['event'] + [ col for col in df2.columns if col != 'event' ] ]\n",
    "    \n",
    "    # merge previous\n",
    "    df = pd.concat((df0,df2))\n",
    "\n",
    "    # Convert the datetime columns to string\n",
    "    df['event_time'] = df['event_time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df['accum_time'] = df['accum_time'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df['period_start'] = df['period_start'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    df['period_end'] = df['period_end'].dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "    # save\n",
    "    df.to_csv(datadir+dbfname,index=False)\n",
    "    \n",
    "    print('Clearing temporary files')\n",
    "    wipe_idx(path01)\n",
    "    wipe_idx(path06)\n",
    "    wipe_idx(path24)\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    # generate and export accumulation maps\n",
    "    generate_figures()\n",
    "\n",
    "    # redefine all_times to start at the next month\n",
    "    all_times = [t for t in all_times if t>times[-1]]\n",
    "\n",
    "    current_time = timemod.strftime(\"%Y-%m-%d %H:%M:%S\", timemod.localtime())\n",
    "\n",
    "    elapsed_time = timemod.time() - code_start\n",
    "    hours, rem = divmod(elapsed_time, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    time_elapsed = f\"Elapsed time: {int(hours):02}:{int(minutes):02}:{int(seconds):02}\"\n",
    "    print(time_elapsed)\n",
    "\n",
    "    # Save to log\n",
    "    with open(log_dir+'log_'+str(sep_dist)+'km_'+code_start_str+'.txt', 'a') as f:\n",
    "        f.writelines('\\n'+current_time)\n",
    "        f.writelines('\\n'+time_elapsed)\n",
    "        f.writelines('\\n'+'Completed up to '+times[-1].strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "        f.writelines('\\n'+'Number of events: '+str(len(df)))\n",
    "        f.writelines('\\n'+'Lat/lon grid version: '+str(latver)+'\\n----------------------------')\n",
    "        \n",
    "    # exit if RAM is exceeding 80% and print/save where to resume after restarting python\n",
    "    ram_percent = psutil.virtual_memory().percent\n",
    "    if (ram_percent > 80) & (len(all_times)>0):\n",
    "        np.save(datadir+'mem_error_log',np.array([all_times[0].strftime(\"%Y-%m-%d %H:%M:%S\"),latver]))\n",
    "        print('ERROR: Memory is almost full! Please restart Python and run code again to resume after setting \"resume_from_memory_error = True\"')\n",
    "        mem_error = True\n",
    "        break\n",
    "    else:\n",
    "        mem_error = False\n",
    "        clear_output()\n",
    "\n",
    "if mem_error:\n",
    "    quit()\n",
    "    \n",
    "else:\n",
    "    with open(log_dir+'log_'+str(sep_dist)+'km_'+code_start_str+'.txt', 'a') as f:\n",
    "        f.writelines('\\nComplete')\n",
    "    print('Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
