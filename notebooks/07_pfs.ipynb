{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dbee3f-f300-4868-9249-f434e3800a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jason Chiappa 2025\n",
    "# Examines the raw 1-hr stage iv files through each event and gathers statistics on the 1 mm/hr precipitation features (PFs)\n",
    "# WARNING: Takes several hours to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b4f60c-8422-4064-aa1e-5870c1b859b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import datetime\n",
    "from datetime import datetime, timedelta\n",
    "from geopy.distance import geodesic\n",
    "import pyproj\n",
    "geodesicp = pyproj.Geod(ellps='WGS84')\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10455da-df55-466a-9651-62601218ba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "#### PLEASE MODIFY AS NEEDED ####\n",
    "##############################################################################################\n",
    "\n",
    "# Separation distance between simultaneous events\n",
    "sep_dist = 250\n",
    "# sep_dist = 100\n",
    "\n",
    "# IMPORTANT!!!\n",
    "# True if resuming (or adding additional data to append to dataset), false if starting from beginning.\n",
    "# Failing to change to True will overwrite all previous data and recalculate the parameters.\n",
    "resume = False\n",
    "\n",
    "# Range of events to run [change if resuming]\n",
    "# Can be 'start', 'end' or event id integer\n",
    "start,end = 'start','end'\n",
    "\n",
    "# Main data directory (containing ere_database_exattrs_[sep_dist].csv)\n",
    "datadir = 'C:/Users/jchiappa/OneDrive - University of Oklahoma/ERE_analysis/v2/data/'\n",
    "\n",
    "# Accumulation map array directory\n",
    "arr_dir = 'E:/Research/EREs/data/array_output/'\n",
    "\n",
    "# directory for lat/lon grids\n",
    "latlon_dir = 'E:/Research/EREs/data/stageiv_latlon/'\n",
    "\n",
    "# st4 1h data directory\n",
    "path = 'E:/Research/EREs/data/stage_iv/01h/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3dc15e-dac5-40a0-8341-1e22aa9b9fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################################\n",
    "#### Leave the stuff below ####\n",
    "##############################################################################################\n",
    "\n",
    "# input database file name\n",
    "dbfname = 'ere_database_prelim_'+str(sep_dist)+'km.csv'\n",
    "\n",
    "# output database file name\n",
    "outputfname = 'ere_database_pfs_'+str(sep_dist)+'km.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74c003c-6cff-4c54-8920-9d0fe3edeb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# degree longitude to distance (km)\n",
    "def dlon_km(event_lat):\n",
    "    coord1 = (event_lat,0)\n",
    "    coord2 = (event_lat,1)\n",
    "    return(geodesic(coord1,coord2).km)\n",
    "\n",
    "# delete unnecessary idx files produced by xarray\n",
    "def wipe_idx(path):\n",
    "    files = [file for file in os.listdir(path) if file.endswith('.idx')]\n",
    "    for file in files:\n",
    "        os.remove(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1deaf-1288-4d09-8136-67c868496432",
   "metadata": {},
   "outputs": [],
   "source": [
    "wipe_idx(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa8d48d-8e47-4f0f-a980-48ebb0b99cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of attribute names\n",
    "attrs = ['event','time','pf_length','lat1','lon1','lat2','lon2','azimuth','pf_points','pf_average']\n",
    "\n",
    "data = {}\n",
    "for attr in attrs:\n",
    "    data[attr] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c7eb92-0434-450b-b064-dc39b73569b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume:\n",
    "    # load existing and save parameters to list to be added to\n",
    "    df0 = pd.read_csv(datadir+outputfname)\n",
    "    # df0['time'] = pd.to_datetime(df0['time'], format=\"%Y%m%d%H\")\n",
    "    # save parameters for events before run period\n",
    "    df0 = df0[df0.event < start]\n",
    "    \n",
    "    for attr in attrs:\n",
    "        data[attr] += df0[attr].to_list()\n",
    "\n",
    "# event database\n",
    "df = pd.read_csv(datadir+dbfname)\n",
    "df['event_time'] = pd.to_datetime(df['event_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df['start_time'] = pd.to_datetime(df['start_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "df['accum_time'] = pd.to_datetime(df['accum_time'], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "if start=='start':\n",
    "    start = df['event'].to_list()[0]\n",
    "if end=='end':\n",
    "    end = df['event'].to_list()[-1]\n",
    "\n",
    "# dataframe with events to analyze\n",
    "df = df[(df.event >= start) & (df.event <= end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4eea97b-a873-4d89-baa9-f4225e6ec31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lists from dataframe\n",
    "events = df.event.to_list()\n",
    "starttimes = df.start_time.to_list()\n",
    "endtimes = df.accum_time.to_list()\n",
    "event_lats = df.lat.to_list()\n",
    "event_lons = df.lon.to_list()\n",
    "latvers = df.latversion.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b039ea5-3aba-4dd3-b7d5-94cf617ce06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups all contiguous points exceeding a threshold surounding the point of max exceedance\n",
    "# input precipitation map array, origin point lat/lon, threshold in mm, search radius in pixels (4km)\n",
    "def group(accum_array,maxlat,maxlon,threshold=1,searchradius=1):\n",
    "    dlon = dlon_km(maxlat)\n",
    "    dlat = 111\n",
    "    \n",
    "    # lat/lon box radius for gathering adjacent/surrounding searchradius grid points only\n",
    "    latrad = (searchradius*4/dlat) + (2/dlat)\n",
    "    lonrad = (searchradius*4/dlon) + (2/dlon)\n",
    "\n",
    "    thresh_exceed = accum_array - threshold\n",
    "\n",
    "    latexceed = lat[thresh_exceed > 0]\n",
    "    lonexceed = lon[thresh_exceed > 0]\n",
    "    accumexceed = accum_array[thresh_exceed > 0]\n",
    "    \n",
    "    max_indexs = np.where((latexceed>maxlat-4/dlat/2) & (latexceed<maxlat+4/dlat/2) &\n",
    "                         (lonexceed>maxlon-4/dlon/2) & (lonexceed<maxlon+4/dlon/2))[0]\n",
    "    \n",
    "    if len(max_indexs) == 0:\n",
    "        return(np.array([]),np.array([]),np.array([]))\n",
    "    \n",
    "    else:\n",
    "        max_index = max_indexs[0]\n",
    "\n",
    "        grouplats = [latexceed[max_index]]\n",
    "        grouplons = [lonexceed[max_index]]\n",
    "        groupvals = [accumexceed[max_index]]\n",
    "\n",
    "        latexceed = np.delete(latexceed,max_index)\n",
    "        lonexceed = np.delete(lonexceed,max_index)\n",
    "        accumexceed = np.delete(accumexceed,max_index)\n",
    "\n",
    "        grouplen = [0,len(grouplats)]\n",
    "\n",
    "        while grouplen[-1] > grouplen[-2]:\n",
    "            for ptidx in range(grouplen[-2],grouplen[-1]):\n",
    "                ptlat = grouplats[ptidx]\n",
    "                ptlon = grouplons[ptidx]\n",
    "                nearidxs = np.where((latexceed>ptlat-latrad) & (latexceed<ptlat+latrad) &\n",
    "                                    (lonexceed>ptlon-lonrad) & (lonexceed<ptlon+lonrad))[0]\n",
    "                if len(nearidxs) > 0:\n",
    "                    for nidx in nearidxs:\n",
    "                        grouplats.append(latexceed[nidx])\n",
    "                        grouplons.append(lonexceed[nidx])\n",
    "                        groupvals.append(accumexceed[nidx])\n",
    "                latexceed = np.delete(latexceed,nearidxs)\n",
    "                lonexceed = np.delete(lonexceed,nearidxs)\n",
    "                accumexceed = np.delete(accumexceed,nearidxs)\n",
    "\n",
    "            grouplen.append(len(grouplats))\n",
    "\n",
    "        return(np.array(grouplats),np.array(grouplons),np.array(groupvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5725565-47c9-4239-bc1d-89a778077d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for nearby PF if not connected to point of max exceedance\n",
    "# return new dummy maxlat and maxlon\n",
    "def search_pf(accum_array,maxlat,maxlon,threshold=1):\n",
    "    dlon = dlon_km(maxlat)\n",
    "    dlat = 111\n",
    "    \n",
    "    # lat/lon box radius for 1 pixel search (4 km grid spacing)\n",
    "    latrad = (4/dlat) + (2/dlat)\n",
    "    lonrad = (4/dlon) + (2/dlon)\n",
    "\n",
    "    thresh_exceed = accum_array - threshold\n",
    "\n",
    "    latexceed = lat[thresh_exceed > 0]\n",
    "    lonexceed = lon[thresh_exceed > 0]\n",
    "    accumexceed = accum_array[thresh_exceed > 0]\n",
    "    \n",
    "    # if fail\n",
    "    maxlat2 = None\n",
    "    maxlon2 = None\n",
    "    \n",
    "    # attempt search within 2 pixels and repeat for higher radii if no exceedance found\n",
    "    for multi in range(1,11):\n",
    "        max_indexs1 = np.where((latexceed>maxlat-multi*latrad) & (latexceed<maxlat+multi*latrad) &\n",
    "                     (lonexceed>maxlon-multi*lonrad) & (lonexceed<maxlon+multi*lonrad))[0]\n",
    "        max_indexs2 = np.where((latexceed>maxlat-(multi+1)*latrad) & (latexceed<maxlat+(multi+1)*latrad) &\n",
    "                     (lonexceed>maxlon-(multi+1)*lonrad) & (lonexceed<maxlon+(multi+1)*lonrad))[0]\n",
    "        \n",
    "        delete = [i for i in range(len(max_indexs2)) if max_indexs2[i] in max_indexs1]\n",
    "        near_idxs = np.delete(max_indexs2,delete)\n",
    "\n",
    "        if len(np.nonzero(accumexceed[near_idxs])[0]) > 0:\n",
    "            # use maximum point in case of multiple\n",
    "            newmaxidx = near_idxs[np.argmax(accumexceed[near_idxs])]\n",
    "            maxlat2 = latexceed[newmaxidx]\n",
    "            maxlon2 = lonexceed[newmaxidx]\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return(maxlat2,maxlon2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef37ba-8847-4b29-b074-56503f0cf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine PFs for each event\n",
    "for event in events:\n",
    "    i = events.index(event)\n",
    "    stime = starttimes[i]\n",
    "    etime = endtimes[i]\n",
    "    lat = np.load(latlon_dir+'stageiv_lats_'+str(int(latvers[i]))+'.npy')\n",
    "    lon = np.load(latlon_dir+'stageiv_lons_'+str(int(latvers[i]))+'.npy')\n",
    "    \n",
    "    # get timestamps for each hour in the event's duration\n",
    "    tstamps = np.arange(stime+pd.Timedelta(1,unit='H'),etime+pd.Timedelta(1,unit='H'),pd.Timedelta(1,unit='H'))\n",
    "    times = [tstamp.item().strftime('%Y%m%d%H') for tstamp in tstamps]\n",
    "    \n",
    "    for time in times:\n",
    "        ds = xr.open_dataset(path+'st4_conus.'+time+'.01h.grb2')\n",
    "        if 'tp' in list(ds.variables):\n",
    "            maxlat = event_lats[i]\n",
    "            maxlon = event_lons[i]\n",
    "    \n",
    "            tp = ds.tp.data\n",
    "\n",
    "            grouplats,grouplons,groupvals = group(tp,maxlat,maxlon)\n",
    "            \n",
    "            if len(grouplats) == 0:\n",
    "                # search for nearest 1 mm/hr PF and redefine starting point\n",
    "                maxlat,maxlon = search_pf(tp,maxlat,maxlon,threshold=1)\n",
    "                \n",
    "                if maxlat==None:\n",
    "                    length,lat1,lon1,lat2,lon2,fwd_azimuth,pf_avg,pf_pts = -999,-999,-999,-999,-999,-999,-999,-999\n",
    "                else:\n",
    "                    grouplats,grouplons,groupvals = group(tp,maxlat,maxlon)\n",
    "                \n",
    "            if len(grouplats) > 0:\n",
    "                lat_argmin = np.nanargmin(grouplats)\n",
    "                lat_argmax = np.nanargmax(grouplats)\n",
    "                lon_argmin = np.nanargmin(grouplons)\n",
    "                lon_argmax = np.nanargmax(grouplons)\n",
    "\n",
    "                lat_minpt = (grouplats[lat_argmin],grouplons[lat_argmin])\n",
    "                lat_maxpt = (grouplats[lat_argmax],grouplons[lat_argmax])\n",
    "                lon_minpt = (grouplats[lon_argmin],grouplons[lon_argmin])\n",
    "                lon_maxpt = (grouplats[lon_argmax],grouplons[lon_argmax])\n",
    "\n",
    "                lat_dist = geodesic(lat_minpt,lat_maxpt).km\n",
    "                lon_dist = geodesic(lon_minpt,lon_maxpt).km\n",
    "\n",
    "                imax = [lat_dist,lon_dist].index(max([lat_dist,lon_dist]))\n",
    "                length = [lat_dist,lon_dist][imax]\n",
    "                lat1 = [lat_minpt[0],lon_minpt[0]][imax]\n",
    "                lon1 = [lat_minpt[1],lon_minpt[1]][imax]\n",
    "                lat2 = [lat_maxpt[0],lon_maxpt[0]][imax]\n",
    "                lon2 = [lat_maxpt[1],lon_maxpt[1]][imax]\n",
    "                \n",
    "                fwd_azimuth,back_azimuth,distance = geodesicp.inv(lon1, lat1, lon2, lat2)\n",
    "                \n",
    "                if fwd_azimuth < 0:\n",
    "                    fwd_azimuth = fwd_azimuth+180\n",
    "                    \n",
    "                # get average value of pf and number of grid points in pf\n",
    "                pf_avg = np.nanmean(groupvals)\n",
    "                pf_pts = len(groupvals)\n",
    "            \n",
    "        else:\n",
    "            length,lat1,lon1,lat2,lon2,fwd_azimuth,pf_avg,pf_pts = -999,-999,-999,-999,-999,-999,-999,-999\n",
    "            \n",
    "        data['event'].append(event)\n",
    "        data['time'].append(time)\n",
    "        data['pf_length'].append(length)\n",
    "        data['lat1'].append(lat1)\n",
    "        data['lon1'].append(lon1)\n",
    "        data['lat2'].append(lat2)\n",
    "        data['lon2'].append(lon2)\n",
    "        data['azimuth'].append(fwd_azimuth)\n",
    "        data['pf_points'].append(pf_pts)\n",
    "        data['pf_average'].append(pf_avg)\n",
    "    \n",
    "    df_pfs = pd.DataFrame(data)\n",
    "    df_pfs.to_csv(datadir+outputfname,index=False)\n",
    "\n",
    "    wipe_idx(path)\n",
    "    \n",
    "    print(str(event)+'/'+str(events[-1]),end='\\r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
